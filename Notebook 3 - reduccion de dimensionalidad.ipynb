{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56412d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "import warnings\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de7ccb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset cargado: (57366, 168)\n",
      "   Hogares: 57,366\n",
      "   Variables: 168\n"
     ]
    }
   ],
   "source": [
    "df_scaled = pd.read_parquet('output/dataset_estandarizado_para_clustering.parquet')\n",
    "print(f\"\\nDataset cargado: {df_scaled.shape}\")\n",
    "print(f\"   Hogares: {df_scaled.shape[0]:,}\")\n",
    "print(f\"   Variables: {df_scaled.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9629a546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: (57366, 168)\n",
      "Features: 168\n"
     ]
    }
   ],
   "source": [
    "# Identificar metadata \n",
    "columnas_metadata = []\n",
    "posibles_metadata = ['id', 'hogar_id', 'zona', 'departamento', 'municipio', \n",
    "                     'cluster', 'nombre_cluster', 'cluster_final']\n",
    "\n",
    "for col in posibles_metadata:\n",
    "    if col in df_scaled.columns:\n",
    "        columnas_metadata.append(col)\n",
    "\n",
    "columnas_features = [c for c in df_scaled.columns if c not in columnas_metadata]\n",
    "\n",
    "print(f\"\\nDataset: {df_scaled.shape}\")\n",
    "print(f\"Features: {len(columnas_features)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac2cd500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "IDENTIFICANDO VARIABLES CON ZERO-INFLATION\n",
      "================================================================================\n",
      "\n",
      "Variables con zero-inflation detectadas: 7\n",
      "\n",
      "Top variables problemáticas:\n",
      "              variable  pct_zeros     varianza    max  n_nonzero\n",
      "ingreso_independientes  88.479239 14448.542727 800.00       6609\n",
      "         monto_remesas  90.938884  4035.678807 400.00       5198\n",
      "                  imnl  89.879022  3926.162750 400.00       5806\n",
      "                 oimed  87.567549   940.276158 204.17       7132\n",
      "                  gmsa  74.265244   562.878634 150.00      14763\n",
      "               ingneto  92.882544   151.801248  91.67       4083\n",
      "                  imes  97.901196   104.986601  90.00       1204\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IDENTIFICANDO VARIABLES CON ZERO-INFLATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "variables_zero_inflated = []\n",
    "\n",
    "for col in columnas_features:\n",
    "    if col in df_scaled.columns:\n",
    "        # Porcentaje de ceros\n",
    "        pct_zeros = (df_scaled[col] == 0).sum() / len(df_scaled) * 100\n",
    "        \n",
    "        if pct_zeros > 50:  # Más del 50% son ceros\n",
    "            varianza = df_scaled[col].var()\n",
    "            if varianza > 100:  # Y tiene alta varianza\n",
    "                variables_zero_inflated.append({\n",
    "                    'variable': col,\n",
    "                    'pct_zeros': pct_zeros,\n",
    "                    'varianza': varianza,\n",
    "                    'max': df_scaled[col].max(),\n",
    "                    'n_nonzero': (df_scaled[col] != 0).sum()\n",
    "                })\n",
    "\n",
    "df_zero_inflated = pd.DataFrame(variables_zero_inflated).sort_values('varianza', ascending=False)\n",
    "\n",
    "print(f\"\\nVariables con zero-inflation detectadas: {len(df_zero_inflated)}\")\n",
    "print(\"\\nTop variables problemáticas:\")\n",
    "print(df_zero_inflated.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04be4359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRANSFORMANDO VARIABLES\n",
      "================================================================================\n",
      "\n",
      "Aplicando transformación log1p a 5 variables:\n",
      "   ingreso_independientes:\n",
      "      Varianza antes:  14448.54\n",
      "      Varianza después: 2.91\n",
      "      Reducción: 100.0%\n",
      "   monto_remesas:\n",
      "      Varianza antes:  4035.68\n",
      "      Varianza después: 1.99\n",
      "      Reducción: 100.0%\n",
      "   imnl:\n",
      "      Varianza antes:  3926.16\n",
      "      Varianza después: 2.05\n",
      "      Reducción: 99.9%\n",
      "   oimed:\n",
      "      Varianza antes:  940.28\n",
      "      Varianza después: 1.60\n",
      "      Reducción: 99.8%\n",
      "   gmsa:\n",
      "      Varianza antes:  562.88\n",
      "      Varianza después: 1.95\n",
      "      Reducción: 99.7%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRANSFORMANDO VARIABLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_transformed = df_scaled.copy()\n",
    "\n",
    "# Transformar las top variables con zero-inflation\n",
    "vars_a_transformar = df_zero_inflated['variable'].head(5).tolist()\n",
    "\n",
    "print(f\"\\nAplicando transformación log1p a {len(vars_a_transformar)} variables:\")\n",
    "\n",
    "for var in vars_a_transformar:\n",
    "    if var in df_transformed.columns:\n",
    "        # Guardar valores originales\n",
    "        original = df_transformed[var].copy()\n",
    "        \n",
    "        # Método simple: log1p directo sobre valores estandarizados\n",
    "        # (funciona porque RobustScaler no cambia la distribución dramáticamente)\n",
    "        df_transformed[var] = np.sign(original) * np.log1p(np.abs(original))\n",
    "        \n",
    "        print(f\"   {var}:\")\n",
    "        print(f\"      Varianza antes:  {original.var():.2f}\")\n",
    "        print(f\"      Varianza después: {df_transformed[var].var():.2f}\")\n",
    "        print(f\"      Reducción: {(1 - df_transformed[var].var()/original.var())*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6453fbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VERIFICAR DISTRIBUCIÓN DE VARIANZA\n",
      "================================================================================\n",
      "\n",
      "Comparación de varianza total:\n",
      "   Original:    24466.32\n",
      "   Transformado: 563.70\n",
      "\n",
      "Variable con mayor varianza:\n",
      "   Original:    59.1% de varianza total\n",
      "   Transformado: 26.9% de varianza total\n",
      "\n",
      "   MEJORA: Reducción de dominancia en 32.1 puntos\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFICAR DISTRIBUCIÓN DE VARIANZA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_original = df_scaled[columnas_features].values\n",
    "X_transformed = df_transformed[columnas_features].values\n",
    "\n",
    "# Varianzas\n",
    "var_original = X_original.var(axis=0)\n",
    "var_transformed = X_transformed.var(axis=0)\n",
    "\n",
    "print(\"\\nComparación de varianza total:\")\n",
    "print(f\"   Original:    {var_original.sum():.2f}\")\n",
    "print(f\"   Transformado: {var_transformed.sum():.2f}\")\n",
    "\n",
    "# Variable dominante\n",
    "max_var_orig = var_original.max()\n",
    "max_var_trans = var_transformed.max()\n",
    "prop_orig = max_var_orig / var_original.sum()\n",
    "prop_trans = max_var_trans / var_transformed.sum()\n",
    "\n",
    "print(f\"\\nVariable con mayor varianza:\")\n",
    "print(f\"   Original:    {prop_orig*100:.1f}% de varianza total\")\n",
    "print(f\"   Transformado: {prop_trans*100:.1f}% de varianza total\")\n",
    "\n",
    "if prop_trans < prop_orig:\n",
    "    print(f\"\\n   MEJORA: Reducción de dominancia en {(prop_orig-prop_trans)*100:.1f} puntos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "356fa33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PCA CON DATOS TRANSFORMADOS\n",
      "================================================================================\n",
      "\n",
      "Componentes para 90% varianza: 11\n",
      "Componentes seleccionados: 20\n",
      "\n",
      "Varianza explicada: 93.97%\n",
      "\n",
      "PC1 (varianza: 29.2%):\n",
      "   Variable con mayor loading: ingneto\n",
      "   Loading: 0.8952\n",
      "   ALERTA: ingneto aún domina\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PCA CON DATOS TRANSFORMADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# PCA completo\n",
    "pca_full = PCA(random_state=42)\n",
    "pca_full.fit(X_transformed)\n",
    "\n",
    "varianza_acumulada = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "n_comps_90 = np.argmax(varianza_acumulada >= 0.90) + 1\n",
    "n_components = max(n_comps_90, 20)\n",
    "\n",
    "print(f\"\\nComponentes para 90% varianza: {n_comps_90}\")\n",
    "print(f\"Componentes seleccionados: {n_components}\")\n",
    "\n",
    "# Aplicar PCA\n",
    "pca = PCA(n_components=n_components, random_state=42)\n",
    "X_pca_transformed = pca.fit_transform(X_transformed)\n",
    "\n",
    "print(f\"\\nVarianza explicada: {pca.explained_variance_ratio_.sum()*100:.2f}%\")\n",
    "\n",
    "# Verificar si una variable aún domina\n",
    "loadings_pc1 = np.abs(pca.components_[0])\n",
    "max_loading = loadings_pc1.max()\n",
    "var_dominante = columnas_features[loadings_pc1.argmax()]\n",
    "\n",
    "print(f\"\\nPC1 (varianza: {pca.explained_variance_ratio_[0]*100:.1f}%):\")\n",
    "print(f\"   Variable con mayor loading: {var_dominante}\")\n",
    "print(f\"   Loading: {max_loading:.4f}\")\n",
    "\n",
    "if max_loading < 0.5:\n",
    "    print(f\"   BUENO: Ninguna variable domina PC1\")\n",
    "else:\n",
    "    print(f\"   ALERTA: {var_dominante} aún domina\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be4ad830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARACIÓN FINAL: 3 ENFOQUES\n",
      "================================================================================\n",
      "\n",
      "1. CLUSTERING EN ESPACIO ORIGINAL (168D):\n",
      "   Silhouette:     0.7392\n",
      "   Davies-Bouldin: 0.5693\n",
      "\n",
      "2. CLUSTERING EN PCA SIN TRANSFORMAR (20D):\n",
      "   Silhouette:     0.7444\n",
      "   Davies-Bouldin: 0.5654\n",
      "\n",
      "3. CLUSTERING EN PCA CON TRANSFORMACIÓN (20D):\n",
      "   Silhouette:     0.4042\n",
      "   Davies-Bouldin: 0.9057\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARACIÓN FINAL: 3 ENFOQUES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "k = 5\n",
    "\n",
    "# 1. Original (sin PCA)\n",
    "print(\"\\n1. CLUSTERING EN ESPACIO ORIGINAL (168D):\")\n",
    "kmeans_orig = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "labels_orig = kmeans_orig.fit_predict(X_original)\n",
    "\n",
    "sil_orig = silhouette_score(X_original, labels_orig, sample_size=10000)\n",
    "db_orig = davies_bouldin_score(X_original, labels_orig)\n",
    "\n",
    "print(f\"   Silhouette:     {sil_orig:.4f}\")\n",
    "print(f\"   Davies-Bouldin: {db_orig:.4f}\")\n",
    "\n",
    "# 2. PCA sin transformar\n",
    "print(f\"\\n2. CLUSTERING EN PCA SIN TRANSFORMAR ({n_components}D):\")\n",
    "pca_sin_trans = PCA(n_components=n_components, random_state=42)\n",
    "X_pca_sin_trans = pca_sin_trans.fit_transform(X_original)\n",
    "\n",
    "kmeans_pca_sin = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "labels_pca_sin = kmeans_pca_sin.fit_predict(X_pca_sin_trans)\n",
    "\n",
    "sil_pca_sin = silhouette_score(X_pca_sin_trans, labels_pca_sin, sample_size=10000)\n",
    "db_pca_sin = davies_bouldin_score(X_pca_sin_trans, labels_pca_sin)\n",
    "\n",
    "print(f\"   Silhouette:     {sil_pca_sin:.4f}\")\n",
    "print(f\"   Davies-Bouldin: {db_pca_sin:.4f}\")\n",
    "\n",
    "# 3. PCA con transformación\n",
    "print(f\"\\n3. CLUSTERING EN PCA CON TRANSFORMACIÓN ({n_components}D):\")\n",
    "kmeans_pca_trans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "labels_pca_trans = kmeans_pca_trans.fit_predict(X_pca_transformed)\n",
    "\n",
    "sil_pca_trans = silhouette_score(X_pca_transformed, labels_pca_trans, sample_size=10000)\n",
    "db_pca_trans = davies_bouldin_score(X_pca_transformed, labels_pca_trans)\n",
    "\n",
    "print(f\"   Silhouette:     {sil_pca_trans:.4f}\")\n",
    "print(f\"   Davies-Bouldin: {db_pca_trans:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6497b8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DECISIÓN FINAL\n",
      "================================================================================\n",
      "\n",
      "MEJOR ENFOQUE: PCA sin transformar\n",
      "Silhouette Score: 0.7434\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TABLA COMPARATIVA:\n",
      "--------------------------------------------------------------------------------\n",
      "Enfoque                          Silhouette   Davies-Bouldin  Dimensiones\n",
      "--------------------------------------------------------------------------------\n",
      "Original                             0.7412           0.5693          168\n",
      "PCA sin transformar                  0.7434           0.5654           20\n",
      "PCA con transformación               0.3956           0.9057           20\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "MEJORA RESPECTO A ORIGINAL:\n",
      "   PCA sin transformar:  +0.29%\n",
      "   PCA con transformación: -46.63%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DECISIÓN FINAL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "scores = {\n",
    "    'Original': sil_orig,\n",
    "    'PCA sin transformar': sil_pca_sin,\n",
    "    'PCA con transformación': sil_pca_trans\n",
    "}\n",
    "\n",
    "mejor_enfoque = max(scores, key=scores.get)\n",
    "mejor_score = scores[mejor_enfoque]\n",
    "\n",
    "print(f\"\\nMEJOR ENFOQUE: {mejor_enfoque}\")\n",
    "print(f\"Silhouette Score: {mejor_score:.4f}\")\n",
    "\n",
    "# Tabla comparativa\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TABLA COMPARATIVA:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Enfoque':<30} {'Silhouette':>12} {'Davies-Bouldin':>16} {'Dimensiones':>12}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Original':<30} {sil_orig:>12.4f} {db_orig:>16.4f} {X_original.shape[1]:>12}\")\n",
    "print(f\"{'PCA sin transformar':<30} {sil_pca_sin:>12.4f} {db_pca_sin:>16.4f} {n_components:>12}\")\n",
    "print(f\"{'PCA con transformación':<30} {sil_pca_trans:>12.4f} {db_pca_trans:>16.4f} {n_components:>12}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Diferencias\n",
    "dif_pca_sin = ((sil_pca_sin - sil_orig) / sil_orig) * 100\n",
    "dif_pca_trans = ((sil_pca_trans - sil_orig) / sil_orig) * 100\n",
    "\n",
    "print(f\"\\nMEJORA RESPECTO A ORIGINAL:\")\n",
    "print(f\"   PCA sin transformar:  {dif_pca_sin:+.2f}%\")\n",
    "print(f\"   PCA con transformación: {dif_pca_trans:+.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20516efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GUARDAR DATASET PARA CLUSTERING\n",
      "================================================================================\n",
      "\n",
      "Usando PCA SIN TRANSFORMAR (mejor Silhouette)\n",
      "\n",
      "Dataset guardado: dataset_final_clustering_pca.parquet\n",
      "   Shape: (57366, 21)\n",
      "   Silhouette: 0.7434\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GUARDAR DATASET PARA CLUSTERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if mejor_enfoque == 'Original':\n",
    "    # Usar dataset original\n",
    "    df_final = df_scaled.copy()\n",
    "    df_final['cluster'] = labels_orig\n",
    "    filename = 'dataset_final_clustering_original.parquet'\n",
    "    print(f\"\\nUsando dataset ORIGINAL (mejor Silhouette)\")\n",
    "    \n",
    "elif mejor_enfoque == 'PCA sin transformar':\n",
    "    # Crear dataframe con PCA sin transformar\n",
    "    columnas_pca = [f'PC{i+1}' for i in range(n_components)]\n",
    "    df_final = pd.DataFrame(X_pca_sin_trans, columns=columnas_pca, index=df_scaled.index)\n",
    "    for col in columnas_metadata:\n",
    "        if col in df_scaled.columns:\n",
    "            df_final[col] = df_scaled[col].values\n",
    "    df_final['cluster'] = labels_pca_sin\n",
    "    filename = 'dataset_final_clustering_pca.parquet'\n",
    "    print(f\"\\nUsando PCA SIN TRANSFORMAR (mejor Silhouette)\")\n",
    "    \n",
    "else:  # PCA con transformación\n",
    "    columnas_pca = [f'PC{i+1}' for i in range(n_components)]\n",
    "    df_final = pd.DataFrame(X_pca_transformed, columns=columnas_pca, index=df_scaled.index)\n",
    "    for col in columnas_metadata:\n",
    "        if col in df_scaled.columns:\n",
    "            df_final[col] = df_scaled[col].values\n",
    "    df_final['cluster'] = labels_pca_trans\n",
    "    filename = 'dataset_final_clustering_pca_transformed.parquet'\n",
    "    print(f\"\\nUsando PCA CON TRANSFORMACIÓN (mejor Silhouette)\")\n",
    "\n",
    "# Guardar\n",
    "df_final.to_parquet(f'output/{filename}', compression='snappy', index=False)\n",
    "\n",
    "print(f\"\\nDataset guardado: {filename}\")\n",
    "print(f\"   Shape: {df_final.shape}\")\n",
    "print(f\"   Silhouette: {mejor_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
